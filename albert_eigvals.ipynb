{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab4ff9fbec9249dcbc7208c994eb3b76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM\n",
        "#tokenizer = AlbertTokenizer.from_pretrained('albert-xlarge-v2')\n",
        "def get_model(model_name):\n",
        "    if model_name == \"llama\":\n",
        "        base_model = \"meta-llama/Llama-2-7b-hf\"\n",
        "    elif model_name == \"llama_chat\":\n",
        "        base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "    elif model_name == \"vicuna\":\n",
        "        base_model = \"lmsys/vicuna-7b-v1.5\"\n",
        "    torch.cuda.empty_cache()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        low_cpu_mem_usage=True, \n",
        "        #device_map=\"auto\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model =  get_model(\"llama\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module: \n",
            "\n",
            "Module: model\n",
            "\n",
            "Module: model.embed_tokens\n",
            "\t  Parameter: weight, Shape: torch.Size([32000, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers\n",
            "\n",
            "Module: model.layers.0\n",
            "\n",
            "Module: model.layers.0.self_attn\n",
            "\n",
            "Module: model.layers.0.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.0.mlp\n",
            "\n",
            "Module: model.layers.0.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.mlp.act_fn\n",
            "\n",
            "Module: model.layers.0.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.0.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1\n",
            "\n",
            "Module: model.layers.1.self_attn\n",
            "\n",
            "Module: model.layers.1.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.1.mlp\n",
            "\n",
            "Module: model.layers.1.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.mlp.act_fn\n",
            "\n",
            "Module: model.layers.1.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.1.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2\n",
            "\n",
            "Module: model.layers.2.self_attn\n",
            "\n",
            "Module: model.layers.2.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.2.mlp\n",
            "\n",
            "Module: model.layers.2.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.mlp.act_fn\n",
            "\n",
            "Module: model.layers.2.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.2.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3\n",
            "\n",
            "Module: model.layers.3.self_attn\n",
            "\n",
            "Module: model.layers.3.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.3.mlp\n",
            "\n",
            "Module: model.layers.3.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.mlp.act_fn\n",
            "\n",
            "Module: model.layers.3.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.3.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4\n",
            "\n",
            "Module: model.layers.4.self_attn\n",
            "\n",
            "Module: model.layers.4.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.4.mlp\n",
            "\n",
            "Module: model.layers.4.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.mlp.act_fn\n",
            "\n",
            "Module: model.layers.4.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.4.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5\n",
            "\n",
            "Module: model.layers.5.self_attn\n",
            "\n",
            "Module: model.layers.5.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.5.mlp\n",
            "\n",
            "Module: model.layers.5.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.mlp.act_fn\n",
            "\n",
            "Module: model.layers.5.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.5.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6\n",
            "\n",
            "Module: model.layers.6.self_attn\n",
            "\n",
            "Module: model.layers.6.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.6.mlp\n",
            "\n",
            "Module: model.layers.6.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.mlp.act_fn\n",
            "\n",
            "Module: model.layers.6.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.6.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7\n",
            "\n",
            "Module: model.layers.7.self_attn\n",
            "\n",
            "Module: model.layers.7.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.7.mlp\n",
            "\n",
            "Module: model.layers.7.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.mlp.act_fn\n",
            "\n",
            "Module: model.layers.7.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.7.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8\n",
            "\n",
            "Module: model.layers.8.self_attn\n",
            "\n",
            "Module: model.layers.8.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.8.mlp\n",
            "\n",
            "Module: model.layers.8.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.mlp.act_fn\n",
            "\n",
            "Module: model.layers.8.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.8.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9\n",
            "\n",
            "Module: model.layers.9.self_attn\n",
            "\n",
            "Module: model.layers.9.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.9.mlp\n",
            "\n",
            "Module: model.layers.9.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.mlp.act_fn\n",
            "\n",
            "Module: model.layers.9.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.9.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10\n",
            "\n",
            "Module: model.layers.10.self_attn\n",
            "\n",
            "Module: model.layers.10.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.10.mlp\n",
            "\n",
            "Module: model.layers.10.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.mlp.act_fn\n",
            "\n",
            "Module: model.layers.10.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.10.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11\n",
            "\n",
            "Module: model.layers.11.self_attn\n",
            "\n",
            "Module: model.layers.11.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.11.mlp\n",
            "\n",
            "Module: model.layers.11.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.mlp.act_fn\n",
            "\n",
            "Module: model.layers.11.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.11.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12\n",
            "\n",
            "Module: model.layers.12.self_attn\n",
            "\n",
            "Module: model.layers.12.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.12.mlp\n",
            "\n",
            "Module: model.layers.12.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.mlp.act_fn\n",
            "\n",
            "Module: model.layers.12.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.12.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13\n",
            "\n",
            "Module: model.layers.13.self_attn\n",
            "\n",
            "Module: model.layers.13.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.13.mlp\n",
            "\n",
            "Module: model.layers.13.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.mlp.act_fn\n",
            "\n",
            "Module: model.layers.13.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.13.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14\n",
            "\n",
            "Module: model.layers.14.self_attn\n",
            "\n",
            "Module: model.layers.14.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.14.mlp\n",
            "\n",
            "Module: model.layers.14.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.mlp.act_fn\n",
            "\n",
            "Module: model.layers.14.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.14.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15\n",
            "\n",
            "Module: model.layers.15.self_attn\n",
            "\n",
            "Module: model.layers.15.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.15.mlp\n",
            "\n",
            "Module: model.layers.15.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.mlp.act_fn\n",
            "\n",
            "Module: model.layers.15.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.15.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16\n",
            "\n",
            "Module: model.layers.16.self_attn\n",
            "\n",
            "Module: model.layers.16.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.16.mlp\n",
            "\n",
            "Module: model.layers.16.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.mlp.act_fn\n",
            "\n",
            "Module: model.layers.16.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.16.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17\n",
            "\n",
            "Module: model.layers.17.self_attn\n",
            "\n",
            "Module: model.layers.17.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.17.mlp\n",
            "\n",
            "Module: model.layers.17.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.mlp.act_fn\n",
            "\n",
            "Module: model.layers.17.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.17.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18\n",
            "\n",
            "Module: model.layers.18.self_attn\n",
            "\n",
            "Module: model.layers.18.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.18.mlp\n",
            "\n",
            "Module: model.layers.18.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.mlp.act_fn\n",
            "\n",
            "Module: model.layers.18.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.18.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19\n",
            "\n",
            "Module: model.layers.19.self_attn\n",
            "\n",
            "Module: model.layers.19.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.19.mlp\n",
            "\n",
            "Module: model.layers.19.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.mlp.act_fn\n",
            "\n",
            "Module: model.layers.19.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.19.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20\n",
            "\n",
            "Module: model.layers.20.self_attn\n",
            "\n",
            "Module: model.layers.20.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.20.mlp\n",
            "\n",
            "Module: model.layers.20.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.mlp.act_fn\n",
            "\n",
            "Module: model.layers.20.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.20.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21\n",
            "\n",
            "Module: model.layers.21.self_attn\n",
            "\n",
            "Module: model.layers.21.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.21.mlp\n",
            "\n",
            "Module: model.layers.21.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.mlp.act_fn\n",
            "\n",
            "Module: model.layers.21.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.21.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22\n",
            "\n",
            "Module: model.layers.22.self_attn\n",
            "\n",
            "Module: model.layers.22.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.22.mlp\n",
            "\n",
            "Module: model.layers.22.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.mlp.act_fn\n",
            "\n",
            "Module: model.layers.22.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.22.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23\n",
            "\n",
            "Module: model.layers.23.self_attn\n",
            "\n",
            "Module: model.layers.23.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.23.mlp\n",
            "\n",
            "Module: model.layers.23.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.mlp.act_fn\n",
            "\n",
            "Module: model.layers.23.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.23.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24\n",
            "\n",
            "Module: model.layers.24.self_attn\n",
            "\n",
            "Module: model.layers.24.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.24.mlp\n",
            "\n",
            "Module: model.layers.24.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.mlp.act_fn\n",
            "\n",
            "Module: model.layers.24.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.24.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25\n",
            "\n",
            "Module: model.layers.25.self_attn\n",
            "\n",
            "Module: model.layers.25.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.25.mlp\n",
            "\n",
            "Module: model.layers.25.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.mlp.act_fn\n",
            "\n",
            "Module: model.layers.25.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.25.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26\n",
            "\n",
            "Module: model.layers.26.self_attn\n",
            "\n",
            "Module: model.layers.26.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.26.mlp\n",
            "\n",
            "Module: model.layers.26.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.mlp.act_fn\n",
            "\n",
            "Module: model.layers.26.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.26.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27\n",
            "\n",
            "Module: model.layers.27.self_attn\n",
            "\n",
            "Module: model.layers.27.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.27.mlp\n",
            "\n",
            "Module: model.layers.27.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.mlp.act_fn\n",
            "\n",
            "Module: model.layers.27.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.27.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28\n",
            "\n",
            "Module: model.layers.28.self_attn\n",
            "\n",
            "Module: model.layers.28.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.28.mlp\n",
            "\n",
            "Module: model.layers.28.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.mlp.act_fn\n",
            "\n",
            "Module: model.layers.28.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.28.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29\n",
            "\n",
            "Module: model.layers.29.self_attn\n",
            "\n",
            "Module: model.layers.29.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.29.mlp\n",
            "\n",
            "Module: model.layers.29.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.mlp.act_fn\n",
            "\n",
            "Module: model.layers.29.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.29.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30\n",
            "\n",
            "Module: model.layers.30.self_attn\n",
            "\n",
            "Module: model.layers.30.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.30.mlp\n",
            "\n",
            "Module: model.layers.30.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.mlp.act_fn\n",
            "\n",
            "Module: model.layers.30.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.30.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31\n",
            "\n",
            "Module: model.layers.31.self_attn\n",
            "\n",
            "Module: model.layers.31.self_attn.q_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.self_attn.k_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.self_attn.v_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.self_attn.o_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.self_attn.rotary_emb\n",
            "\n",
            "Module: model.layers.31.mlp\n",
            "\n",
            "Module: model.layers.31.mlp.gate_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.mlp.up_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([11008, 4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.mlp.down_proj\n",
            "\t  Parameter: weight, Shape: torch.Size([4096, 11008]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.mlp.act_fn\n",
            "\n",
            "Module: model.layers.31.input_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.layers.31.post_attention_layernorm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: model.norm\n",
            "\t  Parameter: weight, Shape: torch.Size([4096]), Requires Grad: True\n",
            "\n",
            "Module: lm_head\n",
            "\t  Parameter: weight, Shape: torch.Size([32000, 4096]), Requires Grad: True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Function to print details of each module and its parameters\n",
        "def print_model_parameters(model):\n",
        "    total = 0\n",
        "    for name, module in model.named_modules():\n",
        "        print(f\"Module: {name}\")\n",
        "        for param_name, param in module.named_parameters(recurse=False):\n",
        "            print(f\"\\t  Parameter: {param_name}, Shape: {param.shape}, Requires Grad: {param.requires_grad}\")\n",
        "            total += np.prod(param.shape.to_list())\n",
        "        print(\"\")\n",
        "\n",
        "# Print model architecture and parameters\n",
        "print_model_parameters(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CcFa4dE58XTd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "\n",
        "# returns Bm, Vm whose meaning is this:\n",
        "# \n",
        "# x_i (t+1) = x_i(t) + \\sum_m \\sum_j 1/Z_{i,m} exp(<x_i, Bm x_j>) Vm x_j\n",
        "#\n",
        "# Here tokens are normalized so that ||x_i||=\\sqrt{d_model} (i.e. not on a unit sphere).\n",
        "#\n",
        "def albert_get_BV(al_model, head_idx=0):\n",
        "    al_transfo = al_model.encoder;\n",
        "    al_layer = al_transfo.albert_layer_groups[0].albert_layers[0];\n",
        "    al_attention = al_layer.attention;\n",
        "    \n",
        "    # These matrices that act on token x (row-vector) by x \\times WQ etc \n",
        "    WQ = al_attention.query.weight.T;\n",
        "    WV = al_attention.value.weight.T;\n",
        "    WK = al_attention.key.weight.T;\n",
        "    D = al_attention.dense.weight.T;\n",
        "    \n",
        "    dk = al_attention.attention_head_size; \n",
        "    dmodel = al_attention.hidden_size;\n",
        "    m = head_idx;\n",
        "    \n",
        "    \n",
        "    WQm = WQ[:, (dk*m):(dk*m + dk)]; WKm = WK[:, (dk*m):(dk*m + dk)]; \n",
        "    WVm = torch.zeros(dmodel,dmodel);\n",
        "    WVm[:, (dk*m):(dk*m + dk)] = WV[:, (dk*m):(dk*m + dk)]; \n",
        "    \n",
        "    # bilinear form matrix\n",
        "    b_mtx = torch.matmul(WQm, WKm.T); b_mtx = 0.5* (b_mtx + b_mtx.T) / math.sqrt(dk);\n",
        "    \n",
        "    # Convert value matrix to a matrix acting on tokens as column vectors:\n",
        "    value_mtx = torch.matmul(WVm, D).T; \n",
        "    \n",
        "    return b_mtx.clone().detach(), value_mtx.clone().detach();\n",
        "\n",
        "\n",
        "def plot_B_spectra(al_model):\n",
        "    assert(al_model.config.num_attention_heads == 16) | (al_model.config.num_attention_heads == 1) ;\n",
        "\n",
        "    print('Note that these matrices act on token vectors normalized to ||x||=sqrt(2048)')\n",
        "    \n",
        "    if al_model.config.num_attention_heads == 16:\n",
        "        heads = 16;\n",
        "        fig, axes = plt.subplots(4, 4);\n",
        "        axes = axes.flatten();\n",
        "    elif al_model.config.num_attention_heads == 1:\n",
        "        heads = 1;\n",
        "        fig, axes = plt.subplots(1);\n",
        "        axes = [axes,];\n",
        "    else:\n",
        "        raise AssertionError('num heads');\n",
        "    \n",
        "    minx = 0; maxx = 0;\n",
        "    \n",
        "    betas = [];\n",
        "    dmodel = al_model.config.hidden_size;\n",
        "    assert(dmodel == 2048);\n",
        "    \n",
        "    for i in range(heads):\n",
        "        B, _ = albert_get_BV(al_model, i);\n",
        "        eigs = torch.linalg.eigvalsh(B); \n",
        "        eigs = eigs[eigs.abs() > 1e-6]; \n",
        "        \n",
        "        if heads == 16:\n",
        "            assert len(eigs) == 256;\n",
        "\n",
        "        axes[i].hist(eigs,bins=40, density=True);\n",
        "        axes[i].set_title(f'head {i}');\n",
        "        axes[i].set_ylim(0,8);\n",
        "        \n",
        "        minx = min(minx, eigs.min());\n",
        "        maxx = max(maxx, eigs.max());\n",
        "        \n",
        "        eff_beta = math.sqrt((B.flatten()**2).sum() * dmodel);\n",
        "        betas += [eff_beta,];\n",
        "        \n",
        "    for i in range(heads):\n",
        "        axes[i].set_xlim(minx,maxx)\n",
        "    \n",
        "    #  This is computed as that beta which would yield the same typical RMSE value (for a pair of indep isotropic token)\n",
        "    #  in the model with exp(<x_i, x_j>\\beta / d_model), where again x_i has N(0,1) coordinates.\n",
        "    print('Effective betas = ', betas)\n",
        "    \n",
        "    \n",
        "def plot_V_spectra(al_model):\n",
        "    print(al_model.config.num_attention_heads )\n",
        "\n",
        "    print('Note that these matrices act on token vectors normalized to ||x||=sqrt(2048)')\n",
        "    \n",
        "    print(al_model.config.num_attention_heads)\n",
        "    heads = al_model.config.num_attention_heads \n",
        "    \n",
        "    minx = 0; maxx = 0;\n",
        "    \n",
        "    dmodel = al_model.config.hidden_size;\n",
        "  \n",
        "    \n",
        "    for i in range(heads):\n",
        "        _, V = albert_get_BV(al_model, i);\n",
        "        eigs = torch.linalg.eigvals(V); \n",
        "        eigs = eigs[eigs.abs() > 1e-4]; \n",
        "        \n",
        "        if heads == 16:\n",
        "            print(f'Head = {i}, non-zero eigs = {len(eigs)}')\n",
        "            #assert len(eigs) == 128;\n",
        "\n",
        "        minx = min(minx, eigs.real.min());\n",
        "        maxx = max(maxx, eigs.real.max());\n",
        "\n",
        "        plt.figure()\n",
        "        #plt.plot(eigs.real, eigs.imag, 'k.');\n",
        "\n",
        "        label_size = 16\n",
        "        plt.rcParams['xtick.labelsize'] = label_size\n",
        "        plt.rcParams['ytick.labelsize'] = label_size\n",
        "\n",
        "        plt.gca().spines['right'].set_visible(False)\n",
        "        plt.gca().spines['top'].set_visible(False)\n",
        "\n",
        "        plt.grid(color='silver', linestyle=':', linewidth=0.15, zorder=3)\n",
        "        plt.gca().set_axisbelow(True)\n",
        "\n",
        "\n",
        "        #print(eigs.real.shape)\n",
        "        plt.scatter([x for x in eigs.real], \n",
        "                    [x for x in eigs.imag], \n",
        "                    color='crimson', \n",
        "                    linewidth=0.75, \n",
        "                    edgecolors='black')\n",
        "        title = 'Eigenvalues of value matrix for head %s' % int(i+1)\n",
        "        plt.title(title);\n",
        "        plt.xlim(-2.25-0.1, 1.5+0.1)\n",
        "        plt.ylim(-1.5-0.1, 1.5+0.1)\n",
        "        plt.gca().set_aspect('equal', adjustable='box')\n",
        "        #axes[i].set_ylim(0,8);\n",
        "        \n",
        "        base_filename = \"eigs\" + \"{}.pdf\".format(i+1)\n",
        "        print(i)\n",
        "        plt.savefig(base_filename, \n",
        "                    format='pdf', \n",
        "                    bbox_inches='tight')\n",
        "        plt.show()\n",
        "        plt.clf()\n",
        "        plt.close()\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U2pedI_S8dFi",
        "outputId": "1acc9fa9-751f-45f5-cafd-48f1f26f489c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got Llama\n",
            "32\n",
            "Note that these matrices act on token vectors normalized to ||x||=sqrt(2048)\n",
            "32\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'LlamaForCausalLM' object has no attribute 'encoder'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot Llama\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_V_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mal_model\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 102\u001b[0m, in \u001b[0;36mplot_V_spectra\u001b[0;34m(al_model)\u001b[0m\n\u001b[1;32m     98\u001b[0m dmodel \u001b[38;5;241m=\u001b[39m al_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size;\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(heads):\n\u001b[0;32m--> 102\u001b[0m     _, V \u001b[38;5;241m=\u001b[39m \u001b[43malbert_get_BV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m    103\u001b[0m     eigs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigvals(V); \n\u001b[1;32m    104\u001b[0m     eigs \u001b[38;5;241m=\u001b[39m eigs[eigs\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-4\u001b[39m]; \n",
            "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36malbert_get_BV\u001b[0;34m(al_model, head_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21malbert_get_BV\u001b[39m(al_model, head_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     al_transfo \u001b[38;5;241m=\u001b[39m \u001b[43mal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m;\n\u001b[1;32m     13\u001b[0m     al_layer \u001b[38;5;241m=\u001b[39m al_transfo\u001b[38;5;241m.\u001b[39malbert_layer_groups[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39malbert_layers[\u001b[38;5;241m0\u001b[39m];\n\u001b[1;32m     14\u001b[0m     al_attention \u001b[38;5;241m=\u001b[39m al_layer\u001b[38;5;241m.\u001b[39mattention;\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'encoder'"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Got Llama\")\n",
        "plot_V_spectra(al_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
